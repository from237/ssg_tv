import requests
from bs4 import BeautifulSoup
import csv
import time
import random
from datetime import date, timedelta, datetime
import urllib3
from collections import Counter
import os
import pandas as pd
import numpy as np
import math
import subprocess  # [í•„ìˆ˜] Git ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ

# ë³´ì•ˆ ê²½ê³  ë„ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# [ì„¤ì •] ìˆ˜ì§‘ ë‚ ì§œ (ìë™ ë¶„í•  ì €ì¥ë¨)
# ==========================================
START_DATE = date(2026, 1, 15)
END_DATE = date(2026, 1, 15)

WEIGHT_FOLDER = "weights"
MASTER_WEIGHT_FILE = "weight_2022_2025.csv"

LOADED_WEIGHTS_MAP = {}
LOADED_FALLBACK_MAP = {}
MASTER_FALLBACK_MAP = {}

# PB ë¸Œëœë“œ ëª©ë¡
PB_BRANDS = ["ì—ë””í‹°ë“œ", "ì—ë””ì…˜S", "ë¸”ë£¨í•", "ì—¬ìœ ", "ì—˜ë¼ì½”ë‹‰"]

CATEGORY_MAP = {
    'ê°€ê³µì‹í’ˆê±´ê°•ì‹í’ˆ': 'ê±´ê°•ì‹í’ˆ', 'íŒ¨ì…˜ì—¬ì„±ì˜ë¥˜': 'ì—¬ì„±ì˜ë¥˜', 'íŒ¨ì…˜ë ˆí¬ì¸ ì˜ë¥˜': 'ë ˆí¬ì¸ ',
    'ì‹ ì„ ì‹í’ˆë†ì‚°ë¬¼': 'ì¼ë°˜ì‹í’ˆ', 'ë·°í‹°ê¸°ì´ˆí™”ì¥í’ˆ': 'ë·°í‹°', 'ë·°í‹°ìƒ‰ì¡°í™”ì¥í’ˆ': 'ë·°í‹°',
    'ê°€ê³µì‹í’ˆëƒ‰ë™ì‹í’ˆ': 'ì¼ë°˜ì‹í’ˆ', 'ê°€ê³µì‹í’ˆì¦‰ì„/í¸ì˜ì‹í’ˆ': 'ì¼ë°˜ì‹í’ˆ', 'íŒ¨ì…˜ë‚¨ì„±ìºì¥¬ì–¼': 'ìºì¥¬ì–¼ë‚¨ì„±',
    'íŒ¨ì…˜ì¡í™”ì‹œê³„/ì¥¬ì–¼ë¦¬': 'ì¡í™”', 'íŒ¨ì…˜ì¡í™”ì¡í™”': 'ì¡í™”', 'ë·°í‹°í—¤ì–´/ë°”ë””ìš©í’ˆ': 'ë·°í‹°',
    'ë¬´í˜•ì„œë¹„ìŠ¤ë³´í—˜ê¸ˆìœµ': 'ë³´í—˜', 'ìƒí™œìš©í’ˆì„¸íƒìš©í’ˆ': 'ìƒí™œìš©í’ˆ', 'íŒ¨ì…˜ì¡í™”ì–¸ë”ì›¨ì–´': 'ì–¸ë”ì›¨ì–´',
    'ê°€ê³µì‹í’ˆì¶•ì‚°ê°€ê³µì‹í’ˆ': 'ì¼ë°˜ì‹í’ˆ', 'íŒ¨ì…˜ë ˆí¬ì¸ ìš©í’ˆ': 'ë ˆí¬ì¸ ', 'ê°€ì „/ë””ì§€í„¸ì£¼ë°©ê°€ì „': 'ì£¼ë°©ê°€ì „',
    'ê°€ê³µì‹í’ˆì¡°ë¯¸ë£Œ': 'ì¼ë°˜ì‹í’ˆ', 'ìƒí™œìš©í’ˆìœ„ìƒìš©í’ˆ': 'ìƒí™œìš©í’ˆ', 'ë¬´í˜•ì„œë¹„ìŠ¤ë Œíƒˆë°ê¸°íƒ€ ì„œë¹„ìŠ¤': 'ë Œíƒˆ',
    'ìƒí™œìš©í’ˆì£¼ë°©ìš©í’ˆ': 'ì£¼ë°©ìš©í’ˆ', 'ìƒí™œìš©í’ˆì²­ì†Œ/ìš•ì‹¤ìš©í’ˆ': 'ìƒí™œìš©í’ˆ', 'íŒ¨ì…˜ë‚¨ì„±í´ë˜ì‹': 'ìºì¥¬ì–¼ë‚¨ì„±',
    'ì‹ ì„ ì‹í’ˆìˆ˜ì‚°ë¬¼': 'ì¼ë°˜ì‹í’ˆ', 'ê°€êµ¬/ì¸í…Œë¦¬ì–´ì¹¨êµ¬ë‹¨í’ˆ': 'ì¹¨êµ¬', 'ë·°í‹°ì´ë¯¸ìš©ê¸°ê¸°': 'ë·°í‹°',
    'íŒ¨ì…˜ìœ ë‹ˆì„¹ìŠ¤': 'ìºì¥¬ì–¼ë‚¨ì„±', 'ìƒí™œìš©í’ˆìƒí™œìš©í’ˆ': 'ìƒí™œìš©í’ˆ', 'ê°€ê³µì‹í’ˆë¹µë¥˜/ë–¡ë¥˜': 'ì¼ë°˜ì‹í’ˆ',
    'ì‹ ì„ ì‹í’ˆì¶•ì‚°ë¬¼': 'ì¼ë°˜ì‹í’ˆ', 'ê°€ê³µì‹í’ˆì ˆì„/ë°œíš¨ì‹í’ˆ': 'ì¼ë°˜ì‹í’ˆ', 'ê°€ê³µì‹í’ˆì–´ìœ¡/ì—°ì‹í’ˆë¥˜': 'ì¼ë°˜ì‹í’ˆ',
    'íŒ¨ì…˜ì¡í™”ì‹ ë°œ': 'ì¡í™”', 'ì‹ ì„ ì‹í’ˆì‹ ì„ ì‹í’ˆì„¸íŠ¸ë¥˜': 'ì¼ë°˜ì‹í’ˆ', 'ìŠ¤í¬ì¸ /ë ˆì €í—¬ìŠ¤': 'ë ˆí¬ì¸ ',
    'ìŠ¤í¬ì¸ /ë ˆì €ê³¨í”„': 'ë ˆí¬ì¸ ', 'ìƒí™œìš©í’ˆì˜ë£Œê¸°ê¸°': 'ìƒí™œê°€ì „', 'ë¬´í˜•ì„œë¹„ìŠ¤ì—¬í–‰/ì˜ˆì•½ì„œë¹„ìŠ¤': 'ì—¬í–‰',
    'ê°€êµ¬/ì¸í…Œë¦¬ì–´ì¹¨ì‹¤ê°€êµ¬': 'ê°€êµ¬', 'ê°€êµ¬/ì¸í…Œë¦¬ì–´ê±°ì‹¤ê°€êµ¬': 'ê°€êµ¬', 'ê°€êµ¬/ì¸í…Œë¦¬ì–´ì¸í…Œë¦¬ì–´ì†Œí’ˆ': 'ì¹¨êµ¬',
    'ê°€êµ¬/ì¸í…Œë¦¬ì–´ì¹¨êµ¬ì„¸íŠ¸': 'ì¹¨êµ¬', 'ê°€ê³µì‹í’ˆìŒë£Œë¥˜': 'ì¼ë°˜ì‹í’ˆ', 'êµìœ¡/ë¬¸í™”ë¬¸êµ¬/ì‚¬ë¬´ìš©í’ˆ': 'ìƒí™œìš©í’ˆ',
    'ìƒí™œìš©í’ˆì˜ë£Œìš©í’ˆ': 'ìƒí™œê°€ì „', 'ê°€ê³µì‹í’ˆê³¼ìë¥˜': 'ì¼ë°˜ì‹í’ˆ', 'ê°€ê³µì‹í’ˆìˆ˜ì‚°ê°€ê³µì‹í’ˆ': 'ì¼ë°˜ì‹í’ˆ',
    'ê°€ì „/ë””ì§€í„¸ìƒí™œê°€ì „': 'ìƒí™œê°€ì „', 'ìŠ¤í¬ì¸ /ë ˆì €ë“±ì‚°': 'ë ˆí¬ì¸ '
}


def determine_md_class(brand, cat1, cat2):
    clean_brand = str(brand).replace(" ", "").strip()
    for pb in PB_BRANDS:
        if pb in clean_brand: return "PB"
    key = str(cat1).strip() + str(cat2).strip()
    return CATEGORY_MAP.get(key, "ê¸°íƒ€")


def init_master_fallback():
    global MASTER_FALLBACK_MAP
    paths = [os.path.join(WEIGHT_FOLDER, MASTER_WEIGHT_FILE), MASTER_WEIGHT_FILE]
    f_path = next((p for p in paths if os.path.exists(p)), None)

    if not f_path:
        print(f"âš ï¸ ê²½ê³ : ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ íŒŒì¼({MASTER_WEIGHT_FILE}) ì—†ìŒ -> ì—†ì„ ì‹œ 100% ì ìš©")
        return

    try:
        try:
            df = pd.read_csv(f_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(f_path, encoding='cp949')

        df.columns = [c.strip().lower() for c in df.columns]
        if 'weight' in df.columns and df['weight'].dtype == object:
            df['weight'] = df['weight'].astype(str).str.replace('%', '')
            df['weight'] = pd.to_numeric(df['weight'], errors='coerce')
            if df['weight'].mean() > 5: df['weight'] /= 100

        df['dt'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.dropna(subset=['dt', 'weight'])
        df['weekday'] = df['dt'].dt.weekday

        df_avg = df.groupby(['weekday', 'hour'])['weight'].mean().reset_index()
        MASTER_FALLBACK_MAP = dict(zip(zip(df_avg['weekday'], df_avg['hour']), df_avg['weight']))
        print(f"âœ… ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ë§ˆìŠ¤í„° íŒŒì¼ ë¡œë”© ì—ëŸ¬: {e}")


def load_weight_file_to_dict(file_name):
    if file_name in LOADED_WEIGHTS_MAP: return LOADED_WEIGHTS_MAP[file_name]

    paths = [os.path.join(WEIGHT_FOLDER, file_name), file_name]
    f_path = next((p for p in paths if os.path.exists(p)), None)
    if not f_path: return None

    try:
        try:
            df = pd.read_csv(f_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(f_path, encoding='cp949')

        df.columns = [c.strip().lower() for c in df.columns]
        if not {'date', 'hour', 'weight'}.issubset(df.columns): return None

        if df['weight'].dtype == object:
            df['weight'] = df['weight'].astype(str).str.replace('%', '')
            df['weight'] = pd.to_numeric(df['weight'], errors='coerce')
            if df['weight'].mean() > 5: df['weight'] /= 100

        df_exact = df.groupby(['date', 'hour'])['weight'].mean().reset_index()
        w_map = dict(zip(zip(df_exact['date'], df_exact['hour']), df_exact['weight']))
        LOADED_WEIGHTS_MAP[file_name] = w_map

        df['dt'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.dropna(subset=['dt', 'weight'])
        df['weekday'] = df['dt'].dt.weekday
        df_fallback = df.groupby(['weekday', 'hour'])['weight'].mean().reset_index()
        f_map = dict(zip(zip(df_fallback['weekday'], df_fallback['hour']), df_fallback['weight']))
        LOADED_FALLBACK_MAP[file_name] = f_map

        return w_map
    except Exception as e:
        return None


def calc_final_weighted_mins(target_date, b_time, simple_mins, channel):
    if simple_mins <= 0: return 0

    year = target_date.year

    if 2022 <= year <= 2025:
        f_name = MASTER_WEIGHT_FILE
    else:
        f_name = f"weight_{target_date.strftime('%Y%m')}.csv"

    csv_rate = None
    w_map = load_weight_file_to_dict(f_name)

    start_hour = int(b_time.split(':')[0])
    weekday = target_date.weekday()

    if w_map:
        d_str = target_date.strftime("%Y-%m-%d")
        csv_rate = w_map.get((d_str, start_hour))
        if csv_rate is None:
            fallback_map = LOADED_FALLBACK_MAP.get(f_name, {})
            csv_rate = fallback_map.get((weekday, start_hour))

    if csv_rate is None:
        csv_rate = MASTER_FALLBACK_MAP.get((weekday, start_hour))

    if csv_rate is None:
        csv_rate = 1.0

    ch_rate = 0.7 if channel == "IPTV" else (0.3 if channel == "CATV" else 1.0)

    # ê°€ì¤‘ë¶„ ê³„ì‚° (9% ìƒí–¥ í¬í•¨)
    base_weighted_mins = simple_mins * csv_rate * ch_rate
    up_weighted_mins = base_weighted_mins * 1.09

    return int(math.ceil(up_weighted_mins))


def calc_duration_minutes(time_str):
    if not time_str or "~" not in time_str: return 0
    try:
        s, e = time_str.split("~")
        fmt = "%H:%M"
        ts = datetime.strptime(s.strip(), fmt)
        te = datetime.strptime(e.strip(), fmt)
        if te < ts: te += timedelta(days=1)
        return int((te - ts).total_seconds() / 60)
    except:
        return 0


# [Git ìë™í™” í•¨ìˆ˜]
def push_to_github():
    try:
        print("\nğŸ™ [Git] ë³€ê²½ ì‚¬í•­ì„ GitHubì— í‘¸ì‹œí•©ë‹ˆë‹¤...")

        subprocess.run(["git", "add", "."], check=True)

        result = subprocess.run(["git", "diff-index", "--quiet", "HEAD", "--"], capture_output=True)

        if result.returncode != 0:
            today_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            commit_message = f"Data Update: {today_str}"

            subprocess.run(["git", "commit", "-m", commit_message], check=True)
            print(f" âœ… ì»¤ë°‹ ì™„ë£Œ: {commit_message}")

            subprocess.run(["git", "push"], check=True)
            print(" ğŸš€ GitHub í‘¸ì‹œ ì„±ê³µ!")
        else:
            print(" âš ï¸ ë³€ê²½ëœ ë°ì´í„°ê°€ ì—†ì–´ í‘¸ì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except subprocess.CalledProcessError as e:
        print(f" âŒ Git ì˜¤ë¥˜: {e}")
        print(" â€» ë¨¼ì € í„°ë¯¸ë„ì—ì„œ 'git remote add origin ...' ì„¤ì •ì„ ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f" âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")


def run():
    print(f"ğŸš€ [ìë™ ë¶„í•  ëª¨ë“œ] ìˆ˜ì§‘ ì‹œì‘: {START_DATE} ~ {END_DATE}")

    init_master_fallback()

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
        "Referer": "https://m.shinsegaetvshopping.com/broadcast/tvschedule",
        "X-Requested-With": "XMLHttpRequest",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Origin": "https://m.shinsegaetvshopping.com"
    })

    headers_list = [
        "ë°©ì†¡ì¼ì", "ë°©ì†¡ì‹œê°„", "ì±„ë„êµ¬ë¶„", "ë‹¨ìˆœë¶„", "ê°€ì¤‘ë¶„",
        "ì•„ì´í…œë¶„ë¥˜1", "ì•„ì´í…œë¶„ë¥˜2", "ì•„ì´í…œë¶„ë¥˜3", "ì•„ì´í…œë¶„ë¥˜4", "ì•„ì´í…œë¶„ë¥˜5",
        "ë¸Œëœë“œ", "ìƒí’ˆëª…", "íŒë§¤ê°€", "í• ì¸ê°€", "í”„ë¡œëª¨ì…˜",
        "ìƒí’ˆID", "ì´ë¯¸ì§€URL", "ìƒì„¸ë§í¬",
        "MDë¶„ë¥˜"
    ]

    delta = (END_DATE - START_DATE).days

    for i in range(delta + 1):
        target_date = START_DATE + timedelta(days=i)
        year = target_date.year
        current_filename = f"{year}data.csv"

        p_date = target_date.strftime("%Y/%m/%d")
        s_date = target_date.strftime("%Y-%m-%d")
        print(f"[{i + 1}] ğŸ“… {s_date} -> ğŸ“‚ {current_filename} ì €ì¥ ì¤‘...", end="")

        try:
            daily_rows = []
            session.cookies.clear()
            url = "https://www.shinsegaetvshopping.com/broadcast/tvschedule-ajax"
            params = {"fromDate": p_date, "tomorrowYn": "N", "_": int(time.time() * 1000)}

            resp = session.get(url, params=params, timeout=20, verify=False)

            if resp.status_code != 200:
                print(f" âŒ ì°¨ë‹¨ë¨ ({resp.status_code})")
                time.sleep(3)
                continue

            soup = BeautifulSoup(resp.text, "html.parser")
            dl_list = soup.select("dl")

            if dl_list:
                times = [dl.select_one("dt > span._time").get_text(strip=True) for dl in dl_list if
                         dl.select_one("dt > span._time")]
                t_cnt = Counter(times)
                t_seen = {}

                for dl in dl_list:
                    tt = dl.select_one("dt > span._time")
                    bt = tt.get_text(strip=True) if tt else ""
                    freq = t_cnt[bt]
                    ch = "ì „ì²´"
                    if freq > 1:
                        seen = t_seen.get(bt, 0)
                        ch = "IPTV" if seen == 0 else "CATV"
                        t_seen[bt] = seen + 1

                    sm = calc_duration_minutes(bt)
                    wm = calc_final_weighted_mins(target_date, bt, sm, ch)

                    cards = dl.select("dd > div.card[data-main='Y']")
                    for card in cards:
                        try:
                            full_cat = card.get("data-gtm-item-category", "")
                            cats = full_cat.split(">") + [""] * 5
                            c1, c2, c3, c4, c5 = cats[:5]
                            brand = card.get("data-gtm-item-brand", "").split('(')[0].strip()
                            name = card.get("data-gtm-item-name", "").strip()
                            pd_val = card.get("data-gtm-item-discount", "0")
                            price = int(str(pd_val).replace(",", "")) if pd_val else 0
                            gid = card.get("data-gtm-item-id", "")
                            link = f"https://www.shinsegaetvshopping.com/display/detail/{gid}" if gid else ""
                            img = card.select_one("img")
                            i_url = img.get("src", "").replace("_wg_", "_s_") if img else ""
                            if i_url.startswith("//"): i_url = "https:" + i_url
                            promo = card.select_one("._promoCharge")
                            p_txt = promo.get_text(strip=True) if promo else ""
                            md_class = determine_md_class(brand, c1, c2)

                            daily_rows.append([
                                s_date, bt, ch, sm, wm,
                                c1, c2, c3, c4, c5,
                                brand, name, price, price, p_txt,
                                gid, i_url, link,
                                md_class
                            ])
                        except:
                            continue

            if daily_rows:
                # [ì¤‘ë³µ ì œê±° ë° ì €ì¥ ë¡œì§ ìˆ˜ì •]
                # 1. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_new = pd.DataFrame(daily_rows, columns=headers_list)

                # 2. ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if os.path.exists(current_filename):
                    try:
                        df_old = pd.read_csv(current_filename, encoding='utf-8-sig')
                    except Exception:
                        df_old = pd.DataFrame(columns=headers_list)

                    # 3. ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•©
                    df_combined = pd.concat([df_old, df_new], ignore_index=True)

                    # 4. ì¤‘ë³µ ì œê±° (ë°©ì†¡ì¼ì, ë°©ì†¡ì‹œê°„, ìƒí’ˆID ê¸°ì¤€, ë§ˆì§€ë§‰ í•­ëª© ìœ ì§€)
                    df_combined.drop_duplicates(subset=['ë°©ì†¡ì¼ì', 'ë°©ì†¡ì‹œê°„', 'ìƒí’ˆID'], keep='last', inplace=True)

                    # 5. íŒŒì¼ ì €ì¥
                    df_combined.to_csv(current_filename, index=False, encoding='utf-8-sig')
                else:
                    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆ ë°ì´í„° ë°”ë¡œ ì €ì¥
                    df_new.to_csv(current_filename, index=False, encoding='utf-8-sig')

                print(f" âœ… {len(daily_rows)}ê±´ ìˆ˜ì§‘ / ì¤‘ë³µì œê±° í›„ ì €ì¥ ì™„ë£Œ")
            else:
                print(f" âš ï¸ ë°ì´í„° ì—†ìŒ")

            time.sleep(random.uniform(2, 5))

        except Exception as e:
            print(f" âŒ ì—ëŸ¬: {e}")

    print(f"\nğŸ‰ ëª¨ë“  ìˆ˜ì§‘ ë° íŒŒì¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # [Git ìë™í™” ì‹¤í–‰]
    push_to_github()


if __name__ == "__main__":
    run()