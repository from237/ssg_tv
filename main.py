import requests
from bs4 import BeautifulSoup
import csv
import time
import random
from datetime import date, timedelta, datetime
import urllib3
from collections import Counter
import os
import pandas as pd
import numpy as np
import math  # Ïò¨Î¶º Ï≤òÎ¶¨Î•º ÏúÑÌï¥ math Î™®Îìà ÏÇ¨Ïö©

# Î≥¥Ïïà Í≤ΩÍ≥† ÎÅÑÍ∏∞
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# [ÏÑ§Ï†ï] ÏàòÏßë ÎÇ†Ïßú (ÏûêÎèô Î∂ÑÌï† Ï†ÄÏû•Îê®)
# ==========================================
START_DATE = date(2026, 1, 8)
END_DATE = date(2026, 1, 11)

WEIGHT_FOLDER = "weights"
MASTER_WEIGHT_FILE = "weight_2022_2025.csv"  # 22~25ÎÖÑ ÌÜµÌï© Î∞è Ìè¥Î∞±Ïö© ÎßàÏä§ÌÑ∞ ÌååÏùº

LOADED_WEIGHTS_MAP = {}
LOADED_FALLBACK_MAP = {}
MASTER_FALLBACK_MAP = {}

# PB Î∏åÎûúÎìú Î™©Î°ù
PB_BRANDS = ["ÏóêÎîîÌã∞Îìú", "ÏóêÎîîÏÖòS", "Î∏îÎ£®Ìïè", "Ïó¨Ïú†", "ÏóòÎùºÏΩîÎãâ"]

CATEGORY_MAP = {
    'Í∞ÄÍ≥µÏãùÌíàÍ±¥Í∞ïÏãùÌíà': 'Í±¥Í∞ïÏãùÌíà', 'Ìå®ÏÖòÏó¨ÏÑ±ÏùòÎ•ò': 'Ïó¨ÏÑ±ÏùòÎ•ò', 'Ìå®ÏÖòÎ†àÌè¨Ï∏†ÏùòÎ•ò': 'Î†àÌè¨Ï∏†',
    'Ïã†ÏÑ†ÏãùÌíàÎÜçÏÇ∞Î¨º': 'ÏùºÎ∞òÏãùÌíà', 'Î∑∞Ìã∞Í∏∞Ï¥àÌôîÏû•Ìíà': 'Î∑∞Ìã∞', 'Î∑∞Ìã∞ÏÉâÏ°∞ÌôîÏû•Ìíà': 'Î∑∞Ìã∞',
    'Í∞ÄÍ≥µÏãùÌíàÎÉâÎèôÏãùÌíà': 'ÏùºÎ∞òÏãùÌíà', 'Í∞ÄÍ≥µÏãùÌíàÏ¶âÏÑù/Ìé∏ÏùòÏãùÌíà': 'ÏùºÎ∞òÏãùÌíà', 'Ìå®ÏÖòÎÇ®ÏÑ±Ï∫êÏ•¨Ïñº': 'Ï∫êÏ•¨ÏñºÎÇ®ÏÑ±',
    'Ìå®ÏÖòÏû°ÌôîÏãúÍ≥Ñ/Ï•¨ÏñºÎ¶¨': 'Ïû°Ìôî', 'Ìå®ÏÖòÏû°ÌôîÏû°Ìôî': 'Ïû°Ìôî', 'Î∑∞Ìã∞Ìó§Ïñ¥/Î∞îÎîîÏö©Ìíà': 'Î∑∞Ìã∞',
    'Î¨¥ÌòïÏÑúÎπÑÏä§Î≥¥ÌóòÍ∏àÏúµ': 'Î≥¥Ìóò', 'ÏÉùÌôúÏö©ÌíàÏÑ∏ÌÉÅÏö©Ìíà': 'ÏÉùÌôúÏö©Ìíà', 'Ìå®ÏÖòÏû°ÌôîÏñ∏ÎçîÏõ®Ïñ¥': 'Ïñ∏ÎçîÏõ®Ïñ¥',
    'Í∞ÄÍ≥µÏãùÌíàÏ∂ïÏÇ∞Í∞ÄÍ≥µÏãùÌíà': 'ÏùºÎ∞òÏãùÌíà', 'Ìå®ÏÖòÎ†àÌè¨Ï∏†Ïö©Ìíà': 'Î†àÌè¨Ï∏†', 'Í∞ÄÏ†Ñ/ÎîîÏßÄÌÑ∏Ï£ºÎ∞©Í∞ÄÏ†Ñ': 'Ï£ºÎ∞©Í∞ÄÏ†Ñ',
    'Í∞ÄÍ≥µÏãùÌíàÏ°∞ÎØ∏Î£å': 'ÏùºÎ∞òÏãùÌíà', 'ÏÉùÌôúÏö©ÌíàÏúÑÏÉùÏö©Ìíà': 'ÏÉùÌôúÏö©Ìíà', 'Î¨¥ÌòïÏÑúÎπÑÏä§Î†åÌÉàÎ∞èÍ∏∞ÌÉÄ ÏÑúÎπÑÏä§': 'Î†åÌÉà',
    'ÏÉùÌôúÏö©ÌíàÏ£ºÎ∞©Ïö©Ìíà': 'Ï£ºÎ∞©Ïö©Ìíà', 'ÏÉùÌôúÏö©ÌíàÏ≤≠ÏÜå/ÏöïÏã§Ïö©Ìíà': 'ÏÉùÌôúÏö©Ìíà', 'Ìå®ÏÖòÎÇ®ÏÑ±ÌÅ¥ÎûòÏãù': 'Ï∫êÏ•¨ÏñºÎÇ®ÏÑ±',
    'Ïã†ÏÑ†ÏãùÌíàÏàòÏÇ∞Î¨º': 'ÏùºÎ∞òÏãùÌíà', 'Í∞ÄÍµ¨/Ïù∏ÌÖåÎ¶¨Ïñ¥Ïπ®Íµ¨Îã®Ìíà': 'Ïπ®Íµ¨', 'Î∑∞Ìã∞Ïù¥ÎØ∏Ïö©Í∏∞Í∏∞': 'Î∑∞Ìã∞',
    'Ìå®ÏÖòÏú†ÎãàÏÑπÏä§': 'Ï∫êÏ•¨ÏñºÎÇ®ÏÑ±', 'ÏÉùÌôúÏö©ÌíàÏÉùÌôúÏö©Ìíà': 'ÏÉùÌôúÏö©Ìíà', 'Í∞ÄÍ≥µÏãùÌíàÎπµÎ•ò/Îñ°Î•ò': 'ÏùºÎ∞òÏãùÌíà',
    'Ïã†ÏÑ†ÏãùÌíàÏ∂ïÏÇ∞Î¨º': 'ÏùºÎ∞òÏãùÌíà', 'Í∞ÄÍ≥µÏãùÌíàÏ†àÏûÑ/Î∞úÌö®ÏãùÌíà': 'ÏùºÎ∞òÏãùÌíà', 'Í∞ÄÍ≥µÏãùÌíàÏñ¥Ïú°/Ïó∞ÏãùÌíàÎ•ò': 'ÏùºÎ∞òÏãùÌíà',
    'Ìå®ÏÖòÏû°ÌôîÏã†Î∞ú': 'Ïû°Ìôî', 'Ïã†ÏÑ†ÏãùÌíàÏã†ÏÑ†ÏãùÌíàÏÑ∏Ìä∏Î•ò': 'ÏùºÎ∞òÏãùÌíà', 'Ïä§Ìè¨Ï∏†/Î†àÏ†ÄÌó¨Ïä§': 'Î†àÌè¨Ï∏†',
    'Ïä§Ìè¨Ï∏†/Î†àÏ†ÄÍ≥®ÌîÑ': 'Î†àÌè¨Ï∏†', 'ÏÉùÌôúÏö©ÌíàÏùòÎ£åÍ∏∞Í∏∞': 'ÏÉùÌôúÍ∞ÄÏ†Ñ', 'Î¨¥ÌòïÏÑúÎπÑÏä§Ïó¨Ìñâ/ÏòàÏïΩÏÑúÎπÑÏä§': 'Ïó¨Ìñâ',
    'Í∞ÄÍµ¨/Ïù∏ÌÖåÎ¶¨Ïñ¥Ïπ®Ïã§Í∞ÄÍµ¨': 'Í∞ÄÍµ¨', 'Í∞ÄÍµ¨/Ïù∏ÌÖåÎ¶¨Ïñ¥Í±∞Ïã§Í∞ÄÍµ¨': 'Í∞ÄÍµ¨', 'Í∞ÄÍµ¨/Ïù∏ÌÖåÎ¶¨Ïñ¥Ïù∏ÌÖåÎ¶¨Ïñ¥ÏÜåÌíà': 'Ïπ®Íµ¨',
    'Í∞ÄÍµ¨/Ïù∏ÌÖåÎ¶¨Ïñ¥Ïπ®Íµ¨ÏÑ∏Ìä∏': 'Ïπ®Íµ¨', 'Í∞ÄÍ≥µÏãùÌíàÏùåÎ£åÎ•ò': 'ÏùºÎ∞òÏãùÌíà', 'ÍµêÏú°/Î¨∏ÌôîÎ¨∏Íµ¨/ÏÇ¨Î¨¥Ïö©Ìíà': 'ÏÉùÌôúÏö©Ìíà',
    'ÏÉùÌôúÏö©ÌíàÏùòÎ£åÏö©Ìíà': 'ÏÉùÌôúÍ∞ÄÏ†Ñ', 'Í∞ÄÍ≥µÏãùÌíàÍ≥ºÏûêÎ•ò': 'ÏùºÎ∞òÏãùÌíà', 'Í∞ÄÍ≥µÏãùÌíàÏàòÏÇ∞Í∞ÄÍ≥µÏãùÌíà': 'ÏùºÎ∞òÏãùÌíà',
    'Í∞ÄÏ†Ñ/ÎîîÏßÄÌÑ∏ÏÉùÌôúÍ∞ÄÏ†Ñ': 'ÏÉùÌôúÍ∞ÄÏ†Ñ', 'Ïä§Ìè¨Ï∏†/Î†àÏ†ÄÎì±ÏÇ∞': 'Î†àÌè¨Ï∏†'
}


def determine_md_class(brand, cat1, cat2):
    clean_brand = str(brand).replace(" ", "").strip()
    for pb in PB_BRANDS:
        if pb in clean_brand: return "PB"
    key = str(cat1).strip() + str(cat2).strip()
    return CATEGORY_MAP.get(key, "Í∏∞ÌÉÄ")


# ÎßàÏä§ÌÑ∞ ÌååÏùº(2022-2025) ÎØ∏Î¶¨ Î°úÎî© (Ìè¥Î∞±Ïö©)
def init_master_fallback():
    global MASTER_FALLBACK_MAP
    paths = [os.path.join(WEIGHT_FOLDER, MASTER_WEIGHT_FILE), MASTER_WEIGHT_FILE]
    f_path = next((p for p in paths if os.path.exists(p)), None)

    if not f_path:
        print(f"‚ö†Ô∏è Í≤ΩÍ≥†: ÎßàÏä§ÌÑ∞ Í∞ÄÏ§ëÏπò ÌååÏùº({MASTER_WEIGHT_FILE}) ÏóÜÏùå -> ÏóÜÏùÑ Ïãú 100% Ï†ÅÏö©")
        return

    try:
        try:
            df = pd.read_csv(f_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(f_path, encoding='cp949')

        df.columns = [c.strip().lower() for c in df.columns]
        if 'weight' in df.columns and df['weight'].dtype == object:
            df['weight'] = df['weight'].astype(str).str.replace('%', '')
            df['weight'] = pd.to_numeric(df['weight'], errors='coerce')
            if df['weight'].mean() > 5: df['weight'] /= 100

        df['dt'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.dropna(subset=['dt', 'weight'])
        df['weekday'] = df['dt'].dt.weekday

        # ÏöîÏùº(weekday)Í≥º ÏãúÍ∞Ñ(hour)Î≥Ñ ÌèâÍ∑†Í∞í Ï†ÄÏû•
        df_avg = df.groupby(['weekday', 'hour'])['weight'].mean().reset_index()
        MASTER_FALLBACK_MAP = dict(zip(zip(df_avg['weekday'], df_avg['hour']), df_avg['weight']))
        print(f"‚úÖ ÎßàÏä§ÌÑ∞ Í∞ÄÏ§ëÏπò Î°úÎî© ÏôÑÎ£å")

    except Exception as e:
        print(f"‚ùå ÎßàÏä§ÌÑ∞ ÌååÏùº Î°úÎî© ÏóêÎü¨: {e}")


def load_weight_file_to_dict(file_name):
    if file_name in LOADED_WEIGHTS_MAP: return LOADED_WEIGHTS_MAP[file_name]

    paths = [os.path.join(WEIGHT_FOLDER, file_name), file_name]
    f_path = next((p for p in paths if os.path.exists(p)), None)
    if not f_path: return None

    try:
        try:
            df = pd.read_csv(f_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(f_path, encoding='cp949')

        df.columns = [c.strip().lower() for c in df.columns]
        if not {'date', 'hour', 'weight'}.issubset(df.columns): return None

        if df['weight'].dtype == object:
            df['weight'] = df['weight'].astype(str).str.replace('%', '')
            df['weight'] = pd.to_numeric(df['weight'], errors='coerce')
            if df['weight'].mean() > 5: df['weight'] /= 100

        df_exact = df.groupby(['date', 'hour'])['weight'].mean().reset_index()
        w_map = dict(zip(zip(df_exact['date'], df_exact['hour']), df_exact['weight']))
        LOADED_WEIGHTS_MAP[file_name] = w_map

        df['dt'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.dropna(subset=['dt', 'weight'])
        df['weekday'] = df['dt'].dt.weekday
        df_fallback = df.groupby(['weekday', 'hour'])['weight'].mean().reset_index()
        f_map = dict(zip(zip(df_fallback['weekday'], df_fallback['hour']), df_fallback['weight']))
        LOADED_FALLBACK_MAP[file_name] = f_map

        return w_map
    except Exception as e:
        return None


def calc_final_weighted_mins(target_date, b_time, simple_mins, channel):
    if simple_mins <= 0: return 0

    year = target_date.year

    # 2022~2025: ÎßàÏä§ÌÑ∞ ÌååÏùº, 2026 Ïù¥ÌõÑ: ÏõîÎ≥Ñ ÌååÏùº
    if 2022 <= year <= 2025:
        f_name = MASTER_WEIGHT_FILE
    else:
        f_name = f"weight_{target_date.strftime('%Y%m')}.csv"

    csv_rate = None

    # 1. ÌååÏùº Î°úÎî© ÏãúÎèÑ
    w_map = load_weight_file_to_dict(f_name)

    start_hour = int(b_time.split(':')[0])
    weekday = target_date.weekday()

    if w_map:
        d_str = target_date.strftime("%Y-%m-%d")
        csv_rate = w_map.get((d_str, start_hour))
        if csv_rate is None:
            fallback_map = LOADED_FALLBACK_MAP.get(f_name, {})
            csv_rate = fallback_map.get((weekday, start_hour))

    # 2. ÌååÏùº/Îß§Ìïë Ïã§Ìå® Ïãú ÎßàÏä§ÌÑ∞ ÌååÏùº Ìè¥Î∞±
    if csv_rate is None:
        csv_rate = MASTER_FALLBACK_MAP.get((weekday, start_hour))

    # 3. Í∑∏ÎûòÎèÑ ÏóÜÏúºÎ©¥ 1.0
    if csv_rate is None:
        csv_rate = 1.0

    ch_rate = 0.7 if channel == "IPTV" else (0.3 if channel == "CATV" else 1.0)

    # [ÏàòÏ†ïÎê®]
    # 1. Í∏∞Î≥∏ Í∞ÄÏ§ëÎ∂Ñ Í≥ÑÏÇ∞
    base_weighted_mins = simple_mins * csv_rate * ch_rate

    # 2. Ï†ÑÏ≤¥ Í∞ÄÏ§ëÎ∂Ñ 9% ÏÉÅÌñ• (1.09 Í≥±ÌïòÍ∏∞)
    up_weighted_mins = base_weighted_mins * 1.09

    # 3. ÏµúÏ¢Ö ÏÜåÏà´Ï†ê Ïò¨Î¶º Ï≤òÎ¶¨ ÌõÑ Ï†ïÏàò Î≥ÄÌôò
    return int(math.ceil(up_weighted_mins))


def calc_duration_minutes(time_str):
    if not time_str or "~" not in time_str: return 0
    try:
        s, e = time_str.split("~")
        fmt = "%H:%M"
        ts = datetime.strptime(s.strip(), fmt)
        te = datetime.strptime(e.strip(), fmt)
        if te < ts: te += timedelta(days=1)
        return int((te - ts).total_seconds() / 60)
    except:
        return 0


def run():
    print(f"üöÄ [ÏûêÎèô Î∂ÑÌï† Î™®Îìú] ÏàòÏßë ÏãúÏûë: {START_DATE} ~ {END_DATE}")

    # ÎßàÏä§ÌÑ∞ Í∞ÄÏ§ëÏπò Î°úÎî© (Ìè¥Î∞± Ï§ÄÎπÑ)
    init_master_fallback()

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
        "Referer": "https://m.shinsegaetvshopping.com/broadcast/tvschedule",
        "X-Requested-With": "XMLHttpRequest",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Origin": "https://m.shinsegaetvshopping.com"
    })

    headers_list = [
        "Î∞©ÏÜ°ÏùºÏûê", "Î∞©ÏÜ°ÏãúÍ∞Ñ", "Ï±ÑÎÑêÍµ¨Î∂Ñ", "Îã®ÏàúÎ∂Ñ", "Í∞ÄÏ§ëÎ∂Ñ",
        "ÏïÑÏù¥ÌÖúÎ∂ÑÎ•ò1", "ÏïÑÏù¥ÌÖúÎ∂ÑÎ•ò2", "ÏïÑÏù¥ÌÖúÎ∂ÑÎ•ò3", "ÏïÑÏù¥ÌÖúÎ∂ÑÎ•ò4", "ÏïÑÏù¥ÌÖúÎ∂ÑÎ•ò5",
        "Î∏åÎûúÎìú", "ÏÉÅÌíàÎ™Ö", "ÌåêÎß§Í∞Ä", "Ìï†Ïù∏Í∞Ä", "ÌîÑÎ°úÎ™®ÏÖò",
        "ÏÉÅÌíàID", "Ïù¥ÎØ∏ÏßÄURL", "ÏÉÅÏÑ∏ÎßÅÌÅ¨",
        "MDÎ∂ÑÎ•ò"
    ]

    delta = (END_DATE - START_DATE).days

    for i in range(delta + 1):
        target_date = START_DATE + timedelta(days=i)
        year = target_date.year
        current_filename = f"{year}data.csv"

        p_date = target_date.strftime("%Y/%m/%d")
        s_date = target_date.strftime("%Y-%m-%d")
        print(f"[{i + 1}] üìÖ {s_date} -> üìÇ {current_filename} Ï†ÄÏû• Ï§ë...", end="")

        try:
            daily_rows = []
            session.cookies.clear()
            url = "https://www.shinsegaetvshopping.com/broadcast/tvschedule-ajax"
            params = {"fromDate": p_date, "tomorrowYn": "N", "_": int(time.time() * 1000)}

            resp = session.get(url, params=params, timeout=20, verify=False)

            if resp.status_code != 200:
                print(f" ‚ùå Ï∞®Îã®Îê® ({resp.status_code})")
                time.sleep(3)
                continue

            soup = BeautifulSoup(resp.text, "html.parser")
            dl_list = soup.select("dl")

            if dl_list:
                times = [dl.select_one("dt > span._time").get_text(strip=True) for dl in dl_list if
                         dl.select_one("dt > span._time")]
                t_cnt = Counter(times)
                t_seen = {}

                for dl in dl_list:
                    tt = dl.select_one("dt > span._time")
                    bt = tt.get_text(strip=True) if tt else ""
                    freq = t_cnt[bt]
                    ch = "Ï†ÑÏ≤¥"
                    if freq > 1:
                        seen = t_seen.get(bt, 0)
                        ch = "IPTV" if seen == 0 else "CATV"
                        t_seen[bt] = seen + 1

                    sm = calc_duration_minutes(bt)

                    # [ÏµúÏ¢Ö Í∞ÄÏ§ëÎ∂Ñ Í≥ÑÏÇ∞ (9% ÏÉÅÌñ• Ìè¨Ìï®)]
                    wm = calc_final_weighted_mins(target_date, bt, sm, ch)

                    cards = dl.select("dd > div.card[data-main='Y']")
                    for card in cards:
                        try:
                            full_cat = card.get("data-gtm-item-category", "")
                            cats = full_cat.split(">") + [""] * 5
                            c1, c2, c3, c4, c5 = cats[:5]
                            brand = card.get("data-gtm-item-brand", "").split('(')[0].strip()
                            name = card.get("data-gtm-item-name", "").strip()
                            pd_val = card.get("data-gtm-item-discount", "0")
                            price = int(str(pd_val).replace(",", "")) if pd_val else 0
                            gid = card.get("data-gtm-item-id", "")
                            link = f"https://www.shinsegaetvshopping.com/display/detail/{gid}" if gid else ""
                            img = card.select_one("img")
                            i_url = img.get("src", "").replace("_wg_", "_s_") if img else ""
                            if i_url.startswith("//"): i_url = "https:" + i_url
                            promo = card.select_one("._promoCharge")
                            p_txt = promo.get_text(strip=True) if promo else ""
                            md_class = determine_md_class(brand, c1, c2)

                            daily_rows.append([
                                s_date, bt, ch, sm, wm,
                                c1, c2, c3, c4, c5,
                                brand, name, price, price, p_txt,
                                gid, i_url, link,
                                md_class
                            ])
                        except:
                            continue

            if daily_rows:
                file_exists = os.path.exists(current_filename)
                mode = 'a' if file_exists else 'w'
                with open(current_filename, mode, newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if not file_exists:
                        writer.writerow(headers_list)
                    writer.writerows(daily_rows)
                print(f" ‚úÖ {len(daily_rows)}Í±¥ ÏôÑÎ£å")
            else:
                print(f" ‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")

            time.sleep(random.uniform(2, 5))

        except Exception as e:
            print(f" ‚ùå ÏóêÎü¨: {e}")

    print(f"\nüéâ Î™®Îì† ÏàòÏßë ÏôÑÎ£å!")


if __name__ == "__main__":
    run()